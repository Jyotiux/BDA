{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e09d82e5",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c66925",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/04 12:49:02 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "25/04/04 12:49:11 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.6261682242990654\n",
      "Decision Tree F1-score: 0.6119050206476854\n",
      "Decision Tree Precision: 0.6453037574532903\n",
      "Decision Tree Recall: 0.6261682242990654\n",
      "Decision Tree AUC-ROC: 0.5911949685534592\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Trying to decide between a mental breakdown or a stress nap...  #LifeChoices #Overthinking #Stressed\"\"  (A funny --> Prediction: Not Stressed\n",
      "\n",
      "Text: Currently --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"That 3 AM realization that the deadline is in 3 hours... ðŸ˜… Time to activate superhuman focus mode. #DeadlineStress #LateNightWork #FocusMode\"\" ðŸ“¸ (Image of a clock showing the time or a shot of your workspace at night)\" --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Stress level: 100. But at least I can laugh about it --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Grateful for all the amazing things happening around me! Learning --> Prediction: Not Stressed\n",
      "\n",
      "Text: ðŸ“¸ (Image of you working or something that represents your current journey)\" --> Prediction: Not Stressed\n",
      "\n",
      "Text: I'm feeling exhausted and can't focus. --> Prediction: Not Stressed\n",
      "\n",
      "Text: I had a peaceful walk by the beach. --> Prediction: Not Stressed\n",
      "\n",
      "Text: Deadlines are piling up, and I feel overwhelmed. --> Prediction: Not Stressed\n",
      "\n",
      "Text: Iâ€™m excited to start my new project! --> Prediction: Not Stressed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"StressDetection\").getOrCreate()\n",
    "\n",
    "# Load training data from HDFS\n",
    "data_path = \"hdfs://localhost:9000/inputs/dreaddit_StressAnalysis-Sheet1.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Clean and preprocess labels\n",
    "df = df.withColumn(\"label\", when(col(\"label\") == \"0\", 0).when(col(\"label\") == \"1\", 1).otherwise(None))\n",
    "df = df.withColumn(\"label\", col(\"label\").cast(\"integer\"))\n",
    "df = df.na.drop(subset=[\"label\"])\n",
    "\n",
    "# Tokenize text data\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "df = tokenizer.transform(df)\n",
    "\n",
    "# Compute TF-IDF\n",
    "hashing_tf = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
    "df = hashing_tf.transform(df)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idf_model = idf.fit(df)  # Fit once and reuse\n",
    "df = idf_model.transform(df)\n",
    "\n",
    "# Prepare dataset\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train Decision Tree model\n",
    "dt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "dt_model = dt.fit(train)\n",
    "\n",
    "# Evaluate\n",
    "predictions = dt_model.transform(test)\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\")\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"f1\")\n",
    "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"weightedPrecision\")\n",
    "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"weightedRecall\")\n",
    "evaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "\n",
    "accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "f1_score = evaluator_f1.evaluate(predictions)\n",
    "precision = evaluator_precision.evaluate(predictions)\n",
    "recall = evaluator_recall.evaluate(predictions)\n",
    "auc = evaluator_auc.evaluate(predictions)\n",
    "\n",
    "print(f\"Decision Tree Accuracy: {accuracy}\")\n",
    "print(f\"Decision Tree F1-score: {f1_score}\")\n",
    "print(f\"Decision Tree Precision: {precision}\")\n",
    "print(f\"Decision Tree Recall: {recall}\")\n",
    "print(f\"Decision Tree AUC-ROC: {auc}\")\n",
    "\n",
    "# **Load New Data for Prediction**\n",
    "prediction_data_path = \"hdfs://localhost:9000/stress_analyse/input_/output_messages.csv\"\n",
    "df_predict = spark.read.csv(prediction_data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Apply same preprocessing steps to new data\n",
    "df_predict = tokenizer.transform(df_predict)\n",
    "df_predict = hashing_tf.transform(df_predict)\n",
    "df_predict = idf_model.transform(df_predict)  # Use trained idf_model\n",
    "\n",
    "# Make Predictions\n",
    "df_predict = dt_model.transform(df_predict).select(\"text\", \"prediction\")\n",
    "\n",
    "# Display results\n",
    "for row in df_predict.collect():\n",
    "    text, prediction = row[\"text\"], row[\"prediction\"]\n",
    "    status = \"Stressed\" if prediction == 1 else \"Not Stressed\"\n",
    "    print(f\"Text: {text} --> Prediction: {status}\\n\")\n",
    "\n",
    "    if prediction == 1:\n",
    "        print(\"\\033[91mALERT: The text is classified as 'Stressed'!\\033[0m\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e67e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae9fb1e0",
   "metadata": {},
   "source": [
    "## Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12b05199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosted Trees Accuracy: 0.5981308411214953\n",
      "Gradient Boosted Trees F1-score: 0.5895139736033284\n",
      "Gradient Boosted Trees Precision: 0.6054021685664792\n",
      "Gradient Boosted Trees Recall: 0.5981308411214953\n",
      "Gradient Boosted Trees AUC-ROC: 0.653214535290007\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Trying to decide between a mental breakdown or a stress nap...  #LifeChoices #Overthinking #Stressed\"\"  (A funny --> Prediction: Not Stressed\n",
      "\n",
      "Text: Currently --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"That 3 AM realization that the deadline is in 3 hours... ðŸ˜… Time to activate superhuman focus mode. #DeadlineStress #LateNightWork #FocusMode\"\" ðŸ“¸ (Image of a clock showing the time or a shot of your workspace at night)\" --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Stress level: 100. But at least I can laugh about it --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Grateful for all the amazing things happening around me! Learning --> Prediction: Not Stressed\n",
      "\n",
      "Text: ðŸ“¸ (Image of you working or something that represents your current journey)\" --> Prediction: Not Stressed\n",
      "\n",
      "Text: I'm feeling exhausted and can't focus. --> Prediction: Not Stressed\n",
      "\n",
      "Text: I had a peaceful walk by the beach. --> Prediction: Not Stressed\n",
      "\n",
      "Text: Deadlines are piling up, and I feel overwhelmed. --> Prediction: Not Stressed\n",
      "\n",
      "Text: Iâ€™m excited to start my new project! --> Prediction: Stressed\n",
      "\n",
      "\u001b[91mALERT: The text is classified as 'Stressed'!\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"StressDetection\").getOrCreate()\n",
    "\n",
    "# Load training data from HDFS\n",
    "data_path = \"hdfs://localhost:9000/inputs/dreaddit_StressAnalysis-Sheet1.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Clean and preprocess labels\n",
    "df = df.withColumn(\"label\", when(col(\"label\") == \"0\", 0).when(col(\"label\") == \"1\", 1).otherwise(None))\n",
    "df = df.withColumn(\"label\", col(\"label\").cast(\"integer\"))\n",
    "df = df.na.drop(subset=[\"label\"])\n",
    "\n",
    "# Tokenize text data\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "df = tokenizer.transform(df)\n",
    "\n",
    "# Compute TF-IDF\n",
    "hashing_tf = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
    "df = hashing_tf.transform(df)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idf_model = idf.fit(df)  # Fit once and reuse\n",
    "df = idf_model.transform(df)\n",
    "\n",
    "# Prepare dataset\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train Gradient Boosted Trees model\n",
    "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "gbt_model = gbt.fit(train)\n",
    "\n",
    "# Evaluate\n",
    "predictions = gbt_model.transform(test)\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\")\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"f1\")\n",
    "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"weightedPrecision\")\n",
    "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"weightedRecall\")\n",
    "evaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "\n",
    "accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "f1_score = evaluator_f1.evaluate(predictions)\n",
    "precision = evaluator_precision.evaluate(predictions)\n",
    "recall = evaluator_recall.evaluate(predictions)\n",
    "auc = evaluator_auc.evaluate(predictions)\n",
    "\n",
    "print(f\"Gradient Boosted Trees Accuracy: {accuracy}\")\n",
    "print(f\"Gradient Boosted Trees F1-score: {f1_score}\")\n",
    "print(f\"Gradient Boosted Trees Precision: {precision}\")\n",
    "print(f\"Gradient Boosted Trees Recall: {recall}\")\n",
    "print(f\"Gradient Boosted Trees AUC-ROC: {auc}\")\n",
    "\n",
    "# **Load New Data for Prediction**\n",
    "prediction_data_path = \"hdfs://localhost:9000/stress_analyse/input_/output_messages.csv\"\n",
    "df_predict = spark.read.csv(prediction_data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Apply same preprocessing steps to new data\n",
    "df_predict = tokenizer.transform(df_predict)\n",
    "df_predict = hashing_tf.transform(df_predict)\n",
    "df_predict = idf_model.transform(df_predict) \n",
    "\n",
    "# Make Predictions\n",
    "df_predict = gbt_model.transform(df_predict).select(\"text\", \"prediction\")\n",
    "\n",
    "# Display results\n",
    "for row in df_predict.collect():\n",
    "    text, prediction = row[\"text\"], row[\"prediction\"]\n",
    "    status = \"Stressed\" if prediction == 1 else \"Not Stressed\"\n",
    "    print(f\"Text: {text} --> Prediction: {status}\\n\")\n",
    "\n",
    "    if prediction == 1:\n",
    "        print(\"\\033[91mALERT: The text is classified as 'Stressed'!\\033[0m\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1e6e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7b7a04b",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d05684c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.6074766355140186\n",
      "SVM F1-score: 0.6043690437825145\n",
      "SVM Precision: 0.6120618737441168\n",
      "SVM Recall: 0.6074766355140186\n",
      "SVM AUC-ROC: 0.6502445842068483\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Trying to decide between a mental breakdown or a stress nap...  #LifeChoices #Overthinking #Stressed\"\"  (A funny --> Prediction: Not Stressed\n",
      "\n",
      "Text: Currently --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"That 3 AM realization that the deadline is in 3 hours... ðŸ˜… Time to activate superhuman focus mode. #DeadlineStress #LateNightWork #FocusMode\"\" ðŸ“¸ (Image of a clock showing the time or a shot of your workspace at night)\" --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Stress level: 100. But at least I can laugh about it --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Grateful for all the amazing things happening around me! Learning --> Prediction: Stressed\n",
      "\n",
      "\u001b[91mALERT: The text is classified as 'Stressed'!\u001b[0m\n",
      "\n",
      "Text: ðŸ“¸ (Image of you working or something that represents your current journey)\" --> Prediction: Stressed\n",
      "\n",
      "\u001b[91mALERT: The text is classified as 'Stressed'!\u001b[0m\n",
      "\n",
      "Text: I'm feeling exhausted and can't focus. --> Prediction: Stressed\n",
      "\n",
      "\u001b[91mALERT: The text is classified as 'Stressed'!\u001b[0m\n",
      "\n",
      "Text: I had a peaceful walk by the beach. --> Prediction: Not Stressed\n",
      "\n",
      "Text: Deadlines are piling up, and I feel overwhelmed. --> Prediction: Stressed\n",
      "\n",
      "\u001b[91mALERT: The text is classified as 'Stressed'!\u001b[0m\n",
      "\n",
      "Text: Iâ€™m excited to start my new project! --> Prediction: Stressed\n",
      "\n",
      "\u001b[91mALERT: The text is classified as 'Stressed'!\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"StressDetection\").getOrCreate()\n",
    "\n",
    "# Load training data from HDFS\n",
    "data_path = \"hdfs://localhost:9000/inputs/dreaddit_StressAnalysis-Sheet1.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Clean and preprocess labels\n",
    "df = df.withColumn(\"label\", when(col(\"label\") == \"0\", 0).when(col(\"label\") == \"1\", 1).otherwise(None))\n",
    "df = df.withColumn(\"label\", col(\"label\").cast(\"integer\"))\n",
    "df = df.na.drop(subset=[\"label\"])\n",
    "\n",
    "# Tokenize text data\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "df = tokenizer.transform(df)\n",
    "\n",
    "# Compute TF-IDF\n",
    "hashing_tf = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
    "df = hashing_tf.transform(df)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idf_model = idf.fit(df)  # Fit once and reuse\n",
    "df = idf_model.transform(df)\n",
    "\n",
    "# Prepare dataset\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train SVM model\n",
    "svm = LinearSVC(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "svm_model = svm.fit(train)\n",
    "\n",
    "# Evaluate\n",
    "predictions = svm_model.transform(test)\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\")\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"f1\")\n",
    "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"weightedPrecision\")\n",
    "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"weightedRecall\")\n",
    "evaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "\n",
    "accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "f1_score = evaluator_f1.evaluate(predictions)\n",
    "precision = evaluator_precision.evaluate(predictions)\n",
    "recall = evaluator_recall.evaluate(predictions)\n",
    "auc = evaluator_auc.evaluate(predictions)\n",
    "\n",
    "print(f\"SVM Accuracy: {accuracy}\")\n",
    "print(f\"SVM F1-score: {f1_score}\")\n",
    "print(f\"SVM Precision: {precision}\")\n",
    "print(f\"SVM Recall: {recall}\")\n",
    "print(f\"SVM AUC-ROC: {auc}\")\n",
    "\n",
    "# **Load New Data for Prediction**\n",
    "prediction_data_path = \"hdfs://localhost:9000/stress_analyse/input_/output_messages.csv\"\n",
    "df_predict = spark.read.csv(prediction_data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Apply same preprocessing steps to new data\n",
    "df_predict = tokenizer.transform(df_predict)\n",
    "df_predict = hashing_tf.transform(df_predict)\n",
    "df_predict = idf_model.transform(df_predict)  \n",
    "\n",
    "# Make Predictions\n",
    "df_predict = svm_model.transform(df_predict).select(\"text\", \"prediction\")\n",
    "\n",
    "# Display results\n",
    "for row in df_predict.collect():\n",
    "    text, prediction = row[\"text\"], row[\"prediction\"]\n",
    "    status = \"Stressed\" if prediction == 1 else \"Not Stressed\"\n",
    "    print(f\"Text: {text} --> Prediction: {status}\\n\")\n",
    "\n",
    "    if prediction == 1:\n",
    "        print(\"\\033[91mALERT: The text is classified as 'Stressed'!\\033[0m\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5611f289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c16f497e",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92ab55fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.6074766355140186\n",
      "Logistic Regression F1-score: 0.6064458493677845\n",
      "Logistic Regression Precision: 0.6092586725803897\n",
      "Logistic Regression Recall: 0.6074766355140186\n",
      "Logistic Regression AUC-ROC: 0.6401118099231305\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Trying to decide between a mental breakdown or a stress nap...  #LifeChoices #Overthinking #Stressed\"\"  (A funny --> Prediction: Not Stressed\n",
      "\n",
      "Text: Currently --> Prediction: Stressed\n",
      "\n",
      "\u001b[91mALERT: The text is classified as 'Stressed'!\u001b[0m\n",
      "\n",
      "Text: \"That 3 AM realization that the deadline is in 3 hours... ðŸ˜… Time to activate superhuman focus mode. #DeadlineStress #LateNightWork #FocusMode\"\" ðŸ“¸ (Image of a clock showing the time or a shot of your workspace at night)\" --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Stress level: 100. But at least I can laugh about it --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Grateful for all the amazing things happening around me! Learning --> Prediction: Stressed\n",
      "\n",
      "\u001b[91mALERT: The text is classified as 'Stressed'!\u001b[0m\n",
      "\n",
      "Text: ðŸ“¸ (Image of you working or something that represents your current journey)\" --> Prediction: Stressed\n",
      "\n",
      "\u001b[91mALERT: The text is classified as 'Stressed'!\u001b[0m\n",
      "\n",
      "Text: I'm feeling exhausted and can't focus. --> Prediction: Stressed\n",
      "\n",
      "\u001b[91mALERT: The text is classified as 'Stressed'!\u001b[0m\n",
      "\n",
      "Text: I had a peaceful walk by the beach. --> Prediction: Stressed\n",
      "\n",
      "\u001b[91mALERT: The text is classified as 'Stressed'!\u001b[0m\n",
      "\n",
      "Text: Deadlines are piling up, and I feel overwhelmed. --> Prediction: Stressed\n",
      "\n",
      "\u001b[91mALERT: The text is classified as 'Stressed'!\u001b[0m\n",
      "\n",
      "Text: Iâ€™m excited to start my new project! --> Prediction: Stressed\n",
      "\n",
      "\u001b[91mALERT: The text is classified as 'Stressed'!\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"StressDetection\").getOrCreate()\n",
    "\n",
    "# Load training data from HDFS\n",
    "data_path = \"hdfs://localhost:9000/inputs/dreaddit_StressAnalysis-Sheet1.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Clean and preprocess labels\n",
    "df = df.withColumn(\"label\", when(col(\"label\") == \"0\", 0).when(col(\"label\") == \"1\", 1).otherwise(None))\n",
    "df = df.withColumn(\"label\", col(\"label\").cast(\"integer\"))\n",
    "df = df.na.drop(subset=[\"label\"])\n",
    "\n",
    "# Tokenize text data\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "df = tokenizer.transform(df)\n",
    "\n",
    "# Compute TF-IDF\n",
    "hashing_tf = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
    "df = hashing_tf.transform(df)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idf_model = idf.fit(df)  # Fit once and reuse\n",
    "df = idf_model.transform(df)\n",
    "\n",
    "# Prepare dataset\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "lr_model = lr.fit(train)\n",
    "\n",
    "# Evaluate\n",
    "predictions = lr_model.transform(test)\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\")\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"f1\")\n",
    "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"weightedPrecision\")\n",
    "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"weightedRecall\")\n",
    "evaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "\n",
    "accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "f1_score = evaluator_f1.evaluate(predictions)\n",
    "precision = evaluator_precision.evaluate(predictions)\n",
    "recall = evaluator_recall.evaluate(predictions)\n",
    "auc = evaluator_auc.evaluate(predictions)\n",
    "\n",
    "print(f\"Logistic Regression Accuracy: {accuracy}\")\n",
    "print(f\"Logistic Regression F1-score: {f1_score}\")\n",
    "print(f\"Logistic Regression Precision: {precision}\")\n",
    "print(f\"Logistic Regression Recall: {recall}\")\n",
    "print(f\"Logistic Regression AUC-ROC: {auc}\")\n",
    "\n",
    "# **Load New Data for Prediction**\n",
    "prediction_data_path = \"hdfs://localhost:9000/stress_analyse/input_/output_messages.csv\"\n",
    "df_predict = spark.read.csv(prediction_data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Apply same preprocessing steps to new data\n",
    "df_predict = tokenizer.transform(df_predict)\n",
    "df_predict = hashing_tf.transform(df_predict)\n",
    "df_predict = idf_model.transform(df_predict)  \n",
    "\n",
    "# Make Predictions\n",
    "df_predict = lr_model.transform(df_predict).select(\"text\", \"prediction\")\n",
    "\n",
    "# Display results\n",
    "for row in df_predict.collect():\n",
    "    text, prediction = row[\"text\"], row[\"prediction\"]\n",
    "    status = \"Stressed\" if prediction == 1 else \"Not Stressed\"\n",
    "    print(f\"Text: {text} --> Prediction: {status}\\n\")\n",
    "\n",
    "    if prediction == 1:\n",
    "        print(\"\\033[91mALERT: The text is classified as 'Stressed'!\\033[0m\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a87644c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38ee52fe",
   "metadata": {},
   "source": [
    "## Hard Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d400317",
   "metadata": {},
   "source": [
    "#### Combine the predictions of multiple classifiers. The algorithm works by first having each classifier make a prediction. The ensembleâ€™s prediction is then simply the majority vote of the individual classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2dbb0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier Accuracy: 0.616822429906542\n",
      "Voting Classifier F1-score: 0.6086063469241039\n",
      "Voting Classifier Precision: 0.6578947368421053\n",
      "Voting Classifier Recall: 0.4716981132075472\n",
      "Voting Classifier AUC-ROC: 0.6154786862334032\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Trying to decide between a mental breakdown or a stress nap...  #LifeChoices #Overthinking #Stressed\"\"  (A funny --> Prediction: Not Stressed\n",
      "\n",
      "Text: Currently --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"That 3 AM realization that the deadline is in 3 hours... ðŸ˜… Time to activate superhuman focus mode. #DeadlineStress #LateNightWork #FocusMode\"\" ðŸ“¸ (Image of a clock showing the time or a shot of your workspace at night)\" --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Stress level: 100. But at least I can laugh about it --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Grateful for all the amazing things happening around me! Learning --> Prediction: Not Stressed\n",
      "\n",
      "Text: ðŸ“¸ (Image of you working or something that represents your current journey)\" --> Prediction: Not Stressed\n",
      "\n",
      "Text: I'm feeling exhausted and can't focus. --> Prediction: Not Stressed\n",
      "\n",
      "Text: I had a peaceful walk by the beach. --> Prediction: Not Stressed\n",
      "\n",
      "Text: Deadlines are piling up, and I feel overwhelmed. --> Prediction: Not Stressed\n",
      "\n",
      "Text: Iâ€™m excited to start my new project! --> Prediction: Stressed\n",
      "\n",
      "\u001b[91mALERT: The text is classified as 'Stressed'!\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier, GBTClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Initialize models\n",
    "dt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "\n",
    "# Train models\n",
    "dt_model = dt.fit(train)\n",
    "gbt_model = gbt.fit(train)\n",
    "lr_model = lr.fit(train)\n",
    "\n",
    "# Get predictions for each model\n",
    "dt_pred = dt_model.transform(test).select(\"features\", \"label\", \"prediction\").withColumnRenamed(\"prediction\", \"dt_pred\")\n",
    "gbt_pred = gbt_model.transform(test).select(\"features\", \"prediction\").withColumnRenamed(\"prediction\", \"gbt_pred\")\n",
    "lr_pred = lr_model.transform(test).select(\"features\", \"prediction\").withColumnRenamed(\"prediction\", \"lr_pred\")\n",
    "\n",
    "# Combine predictions using \"features\" as the join key\n",
    "combined = dt_pred.join(gbt_pred, \"features\").join(lr_pred, \"features\").select(\"features\", \"label\", \"dt_pred\", \"gbt_pred\", \"lr_pred\")\n",
    "\n",
    "# Ensure `label` is of DoubleType\n",
    "combined = combined.withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
    "\n",
    "# Majority voting (hard voting)\n",
    "combined = combined.withColumn(\n",
    "    \"final_prediction\",\n",
    "    when((col(\"dt_pred\") + col(\"gbt_pred\") + col(\"lr_pred\")) >= 2, 1).otherwise(0).cast(DoubleType())\n",
    ")\n",
    "\n",
    "# Evaluate majority vote accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"final_prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(combined)\n",
    "\n",
    "# Compute F1-score, Precision, Recall, AUC-ROC\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"final_prediction\", metricName=\"f1\")\n",
    "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"final_prediction\", metricName=\"precisionByLabel\")\n",
    "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"final_prediction\", metricName=\"recallByLabel\")\n",
    "auc_evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"final_prediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "f1_score = f1_evaluator.evaluate(combined)\n",
    "precision = precision_evaluator.evaluate(combined)\n",
    "recall = recall_evaluator.evaluate(combined)\n",
    "auc_roc = auc_evaluator.evaluate(combined)\n",
    "\n",
    "print(f\"Voting Classifier Accuracy: {accuracy}\")\n",
    "print(f\"Voting Classifier F1-score: {f1_score}\")\n",
    "print(f\"Voting Classifier Precision: {precision}\")\n",
    "print(f\"Voting Classifier Recall: {recall}\")\n",
    "print(f\"Voting Classifier AUC-ROC: {auc_roc}\")\n",
    "\n",
    "# **Make Predictions on New Data**\n",
    "df_predict = spark.read.csv(prediction_data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Ensure text column exists\n",
    "if \"text\" not in df_predict.columns:\n",
    "    print(\"\\033[91mError: 'text' column missing in prediction dataset.\\033[0m\")\n",
    "    exit()\n",
    "\n",
    "# Apply preprocessing steps\n",
    "df_predict = tokenizer.transform(df_predict)\n",
    "df_predict = hashing_tf.transform(df_predict)\n",
    "df_predict = idf_model.transform(df_predict)\n",
    "\n",
    "# Get predictions from each model\n",
    "dt_pred = dt_model.transform(df_predict).select(\"features\", \"prediction\").withColumnRenamed(\"prediction\", \"dt_pred\")\n",
    "gbt_pred = gbt_model.transform(df_predict).select(\"features\", \"prediction\").withColumnRenamed(\"prediction\", \"gbt_pred\")\n",
    "lr_pred = lr_model.transform(df_predict).select(\"features\", \"prediction\").withColumnRenamed(\"prediction\", \"lr_pred\")\n",
    "\n",
    "# Combine predictions for voting\n",
    "df_predict = df_predict.join(dt_pred, \"features\").join(gbt_pred, \"features\").join(lr_pred, \"features\")\n",
    "df_predict = df_predict.withColumn(\n",
    "    \"final_prediction\",\n",
    "    when((col(\"dt_pred\") + col(\"gbt_pred\") + col(\"lr_pred\")) >= 2, 1).otherwise(0).cast(DoubleType())\n",
    ")\n",
    "\n",
    "# Display results\n",
    "for row in df_predict.select(\"text\", \"final_prediction\").collect():\n",
    "    text, final_pred = row[\"text\"], row[\"final_prediction\"]\n",
    "    status = \"Stressed\" if final_pred == 1 else \"Not Stressed\"\n",
    "    \n",
    "    print(f\"Text: {text} --> Prediction: {status}\\n\")\n",
    "    if final_pred == 1:\n",
    "        print(\"\\033[91mALERT: The text is classified as 'Stressed'!\\033[0m\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f57f56e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "324cf460",
   "metadata": {},
   "source": [
    "## Stacking Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8f26c4",
   "metadata": {},
   "source": [
    "#### Stacked generalization consists in stacking the output of individual estimator and use a classifier to compute the final prediction. Stacking allows to use the strength of each individual estimator by using their output as input of a final estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9921e727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Classifier Accuracy: 0.6261682242990654\n",
      "Stacking Classifier F1-score: 0.6251865232074139\n",
      "Stacking Classifier Precision: 0.6101694915254238\n",
      "Stacking Classifier Recall: 0.6792452830188679\n",
      "Stacking Classifier AUC-ROC: 0.626659678546471\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Trying to decide between a mental breakdown or a stress nap...  #LifeChoices #Overthinking #Stressed\"\"  (A funny --> Prediction: Not Stressed\n",
      "\n",
      "Text: Currently --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"That 3 AM realization that the deadline is in 3 hours... ðŸ˜… Time to activate superhuman focus mode. #DeadlineStress #LateNightWork #FocusMode\"\" ðŸ“¸ (Image of a clock showing the time or a shot of your workspace at night)\" --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Stress level: 100. But at least I can laugh about it --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Grateful for all the amazing things happening around me! Learning --> Prediction: Not Stressed\n",
      "\n",
      "Text: ðŸ“¸ (Image of you working or something that represents your current journey)\" --> Prediction: Not Stressed\n",
      "\n",
      "Text: I'm feeling exhausted and can't focus. --> Prediction: Not Stressed\n",
      "\n",
      "Text: I had a peaceful walk by the beach. --> Prediction: Not Stressed\n",
      "\n",
      "Text: Deadlines are piling up, and I feel overwhelmed. --> Prediction: Not Stressed\n",
      "\n",
      "Text: Iâ€™m excited to start my new project! --> Prediction: Not Stressed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Train base models\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=10)\n",
    "rf_model = rf.fit(train)\n",
    "\n",
    "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "gbt_model = gbt.fit(train)\n",
    "\n",
    "# Get predictions from base models\n",
    "rf_pred = rf_model.transform(test).select(\"features\", \"prediction\").withColumnRenamed(\"prediction\", \"rf_pred\")\n",
    "gbt_pred = gbt_model.transform(test).select(\"features\", \"prediction\").withColumnRenamed(\"prediction\", \"gbt_pred\")\n",
    "\n",
    "# Prepare meta-training data (Ensure all required columns exist)\n",
    "meta_train = test.select(\"features\", \"label\").join(rf_pred, \"features\").join(gbt_pred, \"features\")\n",
    "\n",
    "# Assemble meta-features\n",
    "meta_assembler = VectorAssembler(inputCols=[\"rf_pred\", \"gbt_pred\"], outputCol=\"meta_features\")\n",
    "meta_train = meta_assembler.transform(meta_train)\n",
    "\n",
    "# Train meta-classifier (Logistic Regression)\n",
    "meta_lr = LogisticRegression(featuresCol=\"meta_features\", labelCol=\"label\", maxIter=10)\n",
    "meta_model = meta_lr.fit(meta_train)\n",
    "\n",
    "# Evaluate stacking classifier\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(meta_model.transform(meta_train))\n",
    "\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"precisionByLabel\")\n",
    "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"recallByLabel\")\n",
    "auc_evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "f1_score = f1_evaluator.evaluate(meta_model.transform(meta_train))\n",
    "precision = precision_evaluator.evaluate(meta_model.transform(meta_train))\n",
    "recall = recall_evaluator.evaluate(meta_model.transform(meta_train))\n",
    "auc_roc = auc_evaluator.evaluate(meta_model.transform(meta_train))\n",
    "\n",
    "print(f\"Stacking Classifier Accuracy: {accuracy}\")\n",
    "print(f\"Stacking Classifier F1-score: {f1_score}\")\n",
    "print(f\"Stacking Classifier Precision: {precision}\")\n",
    "print(f\"Stacking Classifier Recall: {recall}\")\n",
    "print(f\"Stacking Classifier AUC-ROC: {auc_roc}\")\n",
    "\n",
    "# **Make Predictions on New Data**\n",
    "df_predict = spark.read.csv(prediction_data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Ensure `text` column exists\n",
    "if \"text\" not in df_predict.columns:\n",
    "    print(\"\\033[91mError: 'text' column missing in prediction dataset.\\033[0m\")\n",
    "    exit()\n",
    "\n",
    "# Apply preprocessing steps\n",
    "df_predict = tokenizer.transform(df_predict)\n",
    "df_predict = hashing_tf.transform(df_predict)\n",
    "df_predict = idf_model.transform(df_predict)\n",
    "\n",
    "# Get predictions from base models\n",
    "rf_pred = rf_model.transform(df_predict).select(\"features\", \"prediction\").withColumnRenamed(\"prediction\", \"rf_pred\")\n",
    "gbt_pred = gbt_model.transform(df_predict).select(\"features\", \"prediction\").withColumnRenamed(\"prediction\", \"gbt_pred\")\n",
    "\n",
    "# Prepare data for meta-classifier\n",
    "df_predict = df_predict.join(rf_pred, \"features\").join(gbt_pred, \"features\")\n",
    "df_predict = meta_assembler.transform(df_predict)\n",
    "\n",
    "# Get final predictions\n",
    "final_predictions = meta_model.transform(df_predict)\n",
    "\n",
    "# Display results\n",
    "for row in final_predictions.select(\"text\", \"prediction\").collect():\n",
    "    text, final_pred = row[\"text\"], row[\"prediction\"]\n",
    "    status = \"Stressed\" if final_pred == 1 else \"Not Stressed\"\n",
    "    \n",
    "    print(f\"Text: {text} --> Prediction: {status}\\n\")\n",
    "    if final_pred == 1:\n",
    "        print(\"\\033[91mALERT: The text is classified as 'Stressed'!\\033[0m\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58577ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb108624",
   "metadata": {},
   "source": [
    "## XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0b7c50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='396' max='396' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [396/396 00:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.691000</td>\n",
       "      <td>0.647480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.530900</td>\n",
       "      <td>0.718872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>1.067040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7803030303030303\n",
      "F1-score: 0.7913669064748201\n",
      "Precision: 0.7142857142857143\n",
      "Recall: 0.8870967741935484\n",
      "AUC-ROC: 0.7864055299539171\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Trying to decide between a mental breakdown or a stress nap...  #LifeChoices #Overthinking #Stressed\"\"  (A funny --> Prediction: Not Stressed\n",
      "\n",
      "Text: Currently --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"That 3 AM realization that the deadline is in 3 hours... ðŸ˜… Time to activate superhuman focus mode. #DeadlineStress #LateNightWork #FocusMode\"\" ðŸ“¸ (Image of a clock showing the time or a shot of your workspace at night)\" --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Stress level: 100. But at least I can laugh about it --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Grateful for all the amazing things happening around me! Learning --> Prediction: Not Stressed\n",
      "\n",
      "Text: ðŸ“¸ (Image of you working or something that represents your current journey)\" --> Prediction: Not Stressed\n",
      "\n",
      "Text: I'm feeling exhausted and can't focus. --> Prediction: Stressed\n",
      "\n",
      "\u001b[91mALERT: The text is classified as 'Stressed'!\u001b[0m\n",
      "\n",
      "Text: I had a peaceful walk by the beach. --> Prediction: Not Stressed\n",
      "\n",
      "Text: Deadlines are piling up, and I feel overwhelmed. --> Prediction: Stressed\n",
      "\n",
      "\u001b[91mALERT: The text is classified as 'Stressed'!\u001b[0m\n",
      "\n",
      "Text: Iâ€™m excited to start my new project! --> Prediction: Not Stressed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"StressDetection\").getOrCreate()\n",
    "\n",
    "# Load data from HDFS\n",
    "data_path = \"hdfs://localhost:9000/inputs/dreaddit_StressAnalysis-Sheet1.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Clean and preprocess labels\n",
    "df = df.withColumn(\"label\", when(col(\"label\") == \"0\", 0).when(col(\"label\") == \"1\", 1).otherwise(None))\n",
    "df = df.withColumn(\"label\", col(\"label\").cast(\"integer\"))\n",
    "df = df.na.drop(subset=[\"label\"])\n",
    "\n",
    "# Convert Spark DataFrame to Pandas\n",
    "data_pd = df.select(\"text\", \"label\").toPandas()\n",
    "\n",
    "# Choose XLNet model\n",
    "model_name = \"xlnet-base-cased\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "# Tokenize text data\n",
    "encodings = tokenizer(list(data_pd[\"text\"]), truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "labels = list(data_pd[\"label\"])\n",
    "\n",
    "# Create dataset class\n",
    "class StressDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "dataset = StressDataset(encodings, labels)\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Load the XLNet model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,  \n",
    "    per_device_eval_batch_size=4,  \n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True \n",
    ")\n",
    "\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "results = trainer.evaluate()\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = torch.argmax(torch.tensor(predictions.predictions), dim=-1).tolist()\n",
    "y_true = [data[\"labels\"].item() for data in test_dataset]\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "auc_roc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"F1-score: {f1}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"AUC-ROC: {auc_roc}\")\n",
    "\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Prediction function\n",
    "def predict_stress(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    pred_label = torch.argmax(probs).item()\n",
    "    return \"Stressed\" if pred_label == 1 else \"Not Stressed\"\n",
    "\n",
    "# **Load New Data for Prediction**\n",
    "prediction_data_path = \"hdfs://localhost:9000/stress_analyse/input_/output_messages.csv\"\n",
    "df_predict = spark.read.csv(prediction_data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Convert Spark DataFrame to Pandas for easier processing\n",
    "prediction_pd = df_predict.select(\"text\").toPandas()\n",
    "\n",
    "# **Make Predictions on the New Data**\n",
    "for text in prediction_pd[\"text\"]:\n",
    "    prediction = predict_stress(text)\n",
    "    print(f\"Text: {text} --> Prediction: {prediction}\\n\")\n",
    "\n",
    "    if prediction == \"Stressed\":\n",
    "        print(\"\\033[91mALERT: The text is classified as 'Stressed'!\\033[0m\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28d06122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLNetForSequenceClassification(\n",
       "  (transformer): XLNetModel(\n",
       "    (word_embedding): Embedding(32000, 768)\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation_function): GELUActivation()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (sequence_summary): SequenceSummary(\n",
       "    (summary): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "    (first_dropout): Identity()\n",
       "    (last_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (logits_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373b1c54",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "985a19ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='198' max='198' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [198/198 00:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.518986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.574500</td>\n",
       "      <td>0.437745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.574500</td>\n",
       "      <td>0.445013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8333\n",
      "F1-score: 0.8382\n",
      "Precision: 0.8507\n",
      "Recall: 0.8261\n",
      "AUC-ROC: 0.8337\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Trying to decide between a mental breakdown or a stress nap...  #LifeChoices #Overthinking #Stressed\"\"  (A funny --> Prediction: Not Stressed\n",
      "\n",
      "Text: Currently --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"That 3 AM realization that the deadline is in 3 hours... ðŸ˜… Time to activate superhuman focus mode. #DeadlineStress #LateNightWork #FocusMode\"\" ðŸ“¸ (Image of a clock showing the time or a shot of your workspace at night)\" --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Stress level: 100. But at least I can laugh about it --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Grateful for all the amazing things happening around me! Learning --> Prediction: Not Stressed\n",
      "\n",
      "Text: ðŸ“¸ (Image of you working or something that represents your current journey)\" --> Prediction: Not Stressed\n",
      "\n",
      "Text: I'm feeling exhausted and can't focus. --> Prediction: Stressed\n",
      "\n",
      "\u001b[91mALERT: The text is classified as 'Stressed'!\u001b[0m\n",
      "\n",
      "Text: I had a peaceful walk by the beach. --> Prediction: Not Stressed\n",
      "\n",
      "Text: Deadlines are piling up, and I feel overwhelmed. --> Prediction: Stressed\n",
      "\n",
      "\u001b[91mALERT: The text is classified as 'Stressed'!\u001b[0m\n",
      "\n",
      "Text: Iâ€™m excited to start my new project! --> Prediction: Not Stressed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"StressDetection\").getOrCreate()\n",
    "\n",
    "# Load data from HDFS\n",
    "data_path = \"hdfs://localhost:9000/inputs/dreaddit_StressAnalysis-Sheet1.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Clean and preprocess labels\n",
    "df = df.withColumn(\"label\", when(col(\"label\") == \"0\", 0).when(col(\"label\") == \"1\", 1).otherwise(None))\n",
    "df = df.withColumn(\"label\", col(\"label\").cast(\"integer\"))\n",
    "df = df.na.drop(subset=[\"label\"])\n",
    "\n",
    "# Convert Spark DataFrame to Pandas\n",
    "data_pd = df.select(\"text\", \"label\").toPandas()\n",
    "\n",
    "# Choose a model\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize text data\n",
    "encodings = tokenizer(list(data_pd[\"text\"]), truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "labels = list(data_pd[\"label\"])\n",
    "\n",
    "# Create dataset class\n",
    "class StressDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "dataset = StressDataset(encodings, labels)\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,  \n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True  \n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# **Evaluate the model**\n",
    "results = trainer.predict(test_dataset)\n",
    "y_pred = torch.argmax(torch.tensor(results.predictions), dim=-1).tolist()\n",
    "y_true = [data['labels'].item() for data in test_dataset]\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "auc_roc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# **Prediction function**\n",
    "def predict_stress(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    pred_label = torch.argmax(probs).item()\n",
    "    return \"Stressed\" if pred_label == 1 else \"Not Stressed\"\n",
    "\n",
    "# **Load New Data for Prediction**\n",
    "prediction_data_path = \"hdfs://localhost:9000/stress_analyse/input_/output_messages.csv\"\n",
    "df_predict = spark.read.csv(prediction_data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Convert Spark DataFrame to Pandas for easier processing\n",
    "prediction_pd = df_predict.select(\"text\").toPandas()\n",
    "\n",
    "# **Make Predictions on the New Data**\n",
    "for text in prediction_pd[\"text\"]:\n",
    "    prediction = predict_stress(text)\n",
    "    print(f\"Text: {text} --> Prediction: {prediction}\\n\")\n",
    "\n",
    "    if prediction == \"Stressed\":\n",
    "        print(\"\\033[91mALERT: The text is classified as 'Stressed'!\\033[0m\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d68800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b682da66",
   "metadata": {},
   "source": [
    "## DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6e9d01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='198' max='198' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [198/198 00:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.516546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.566200</td>\n",
       "      <td>0.407399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.566200</td>\n",
       "      <td>0.429251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7652\n",
      "F1-score: 0.7669\n",
      "Precision: 0.7183\n",
      "Recall: 0.8226\n",
      "AUC-ROC: 0.7684\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Trying to decide between a mental breakdown or a stress nap...  #LifeChoices #Overthinking #Stressed\"\"  (A funny --> Prediction: Not Stressed\n",
      "\n",
      "Text: Currently --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"That 3 AM realization that the deadline is in 3 hours... ðŸ˜… Time to activate superhuman focus mode. #DeadlineStress #LateNightWork #FocusMode\"\" ðŸ“¸ (Image of a clock showing the time or a shot of your workspace at night)\" --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Stress level: 100. But at least I can laugh about it --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Grateful for all the amazing things happening around me! Learning --> Prediction: Not Stressed\n",
      "\n",
      "Text: ðŸ“¸ (Image of you working or something that represents your current journey)\" --> Prediction: Not Stressed\n",
      "\n",
      "Text: I'm feeling exhausted and can't focus. --> Prediction: Stressed\n",
      "\n",
      "\u001b[91mALERT: The text is classified as 'Stressed'!\u001b[0m\n",
      "\n",
      "Text: I had a peaceful walk by the beach. --> Prediction: Not Stressed\n",
      "\n",
      "Text: Deadlines are piling up, and I feel overwhelmed. --> Prediction: Stressed\n",
      "\n",
      "\u001b[91mALERT: The text is classified as 'Stressed'!\u001b[0m\n",
      "\n",
      "Text: Iâ€™m excited to start my new project! --> Prediction: Not Stressed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"StressDetection\").getOrCreate()\n",
    "\n",
    "# Load data from HDFS\n",
    "data_path = \"hdfs://localhost:9000/inputs/dreaddit_StressAnalysis-Sheet1.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Clean and preprocess labels\n",
    "df = df.withColumn(\"label\", when(col(\"label\") == \"0\", 0).when(col(\"label\") == \"1\", 1).otherwise(None))\n",
    "df = df.withColumn(\"label\", col(\"label\").cast(\"integer\"))\n",
    "df = df.na.drop(subset=[\"label\"])\n",
    "\n",
    "# Convert Spark DataFrame to Pandas\n",
    "data_pd = df.select(\"text\", \"label\").toPandas()\n",
    "\n",
    "# Choose a model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize text data\n",
    "encodings = tokenizer(list(data_pd[\"text\"]), truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "labels = list(data_pd[\"label\"])\n",
    "\n",
    "# Create dataset class\n",
    "class StressDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "dataset = StressDataset(encodings, labels)\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,  # Reduce batch size for memory efficiency\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True  # Use mixed precision for better performance\n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# **Evaluate the model**\n",
    "results = trainer.predict(test_dataset)\n",
    "y_pred = torch.argmax(torch.tensor(results.predictions), dim=-1).tolist()\n",
    "y_true = [data['labels'].item() for data in test_dataset]\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "auc_roc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# **Prediction function**\n",
    "def predict_stress(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    pred_label = torch.argmax(probs).item()\n",
    "    return \"Stressed\" if pred_label == 1 else \"Not Stressed\"\n",
    "\n",
    "# **Load New Data for Prediction**\n",
    "prediction_data_path = \"hdfs://localhost:9000/stress_analyse/input_/output_messages.csv\"\n",
    "df_predict = spark.read.csv(prediction_data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Convert Spark DataFrame to Pandas for easier processing\n",
    "prediction_pd = df_predict.select(\"text\").toPandas()\n",
    "\n",
    "# **Make Predictions on the New Data**\n",
    "for text in prediction_pd[\"text\"]:\n",
    "    prediction = predict_stress(text)\n",
    "    print(f\"Text: {text} --> Prediction: {prediction}\\n\")\n",
    "\n",
    "    if prediction == \"Stressed\":\n",
    "        print(\"\\033[91mALERT: The text is classified as 'Stressed'!\\033[0m\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62f2ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fabd83f",
   "metadata": {},
   "source": [
    "## RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9161addc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='198' max='198' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [198/198 00:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.543940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.614600</td>\n",
       "      <td>0.448078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.614600</td>\n",
       "      <td>0.521296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8258\n",
      "F1-score: 0.8296\n",
      "Precision: 0.7671\n",
      "Recall: 0.9032\n",
      "AUC-ROC: 0.8302\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Trying to decide between a mental breakdown or a stress nap...  #LifeChoices #Overthinking #Stressed\"\"  (A funny --> Prediction: Stressed\n",
      "\n",
      "\u001b[91mALERT: The text is classified as 'Stressed'!\u001b[0m\n",
      "\n",
      "Text: Currently --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"That 3 AM realization that the deadline is in 3 hours... ðŸ˜… Time to activate superhuman focus mode. #DeadlineStress #LateNightWork #FocusMode\"\" ðŸ“¸ (Image of a clock showing the time or a shot of your workspace at night)\" --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Stress level: 100. But at least I can laugh about it --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Grateful for all the amazing things happening around me! Learning --> Prediction: Not Stressed\n",
      "\n",
      "Text: ðŸ“¸ (Image of you working or something that represents your current journey)\" --> Prediction: Not Stressed\n",
      "\n",
      "Text: I'm feeling exhausted and can't focus. --> Prediction: Stressed\n",
      "\n",
      "\u001b[91mALERT: The text is classified as 'Stressed'!\u001b[0m\n",
      "\n",
      "Text: I had a peaceful walk by the beach. --> Prediction: Not Stressed\n",
      "\n",
      "Text: Deadlines are piling up, and I feel overwhelmed. --> Prediction: Stressed\n",
      "\n",
      "\u001b[91mALERT: The text is classified as 'Stressed'!\u001b[0m\n",
      "\n",
      "Text: Iâ€™m excited to start my new project! --> Prediction: Not Stressed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"StressDetection\").getOrCreate()\n",
    "\n",
    "# Load data from HDFS\n",
    "data_path = \"hdfs://localhost:9000/inputs/dreaddit_StressAnalysis-Sheet1.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Clean and preprocess labels\n",
    "df = df.withColumn(\"label\", when(col(\"label\") == \"0\", 0).when(col(\"label\") == \"1\", 1).otherwise(None))\n",
    "df = df.withColumn(\"label\", col(\"label\").cast(\"integer\"))\n",
    "df = df.na.drop(subset=[\"label\"])\n",
    "\n",
    "# Convert Spark DataFrame to Pandas\n",
    "data_pd = df.select(\"text\", \"label\").toPandas()\n",
    "\n",
    "# Choose a model\n",
    "model_name = \"roberta-base\"  # Using RoBERTa for improved performance\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize text data\n",
    "encodings = tokenizer(list(data_pd[\"text\"]), truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "labels = list(data_pd[\"label\"])\n",
    "\n",
    "# Create dataset class\n",
    "class StressDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "dataset = StressDataset(encodings, labels)\n",
    "\n",
    "# Split dataset into training and testing\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,  # Reduced batch size for memory efficiency\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True  # Mixed precision for better performance\n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# **Evaluate the model**\n",
    "results = trainer.predict(test_dataset)\n",
    "y_pred = torch.argmax(torch.tensor(results.predictions), dim=-1).tolist()\n",
    "y_true = [data['labels'].item() for data in test_dataset]\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "auc_roc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "# Display results\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# **Prediction function**\n",
    "def predict_stress(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    pred_label = torch.argmax(probs).item()\n",
    "    return \"Stressed\" if pred_label == 1 else \"Not Stressed\"\n",
    "\n",
    "# **Load New Data for Prediction**\n",
    "prediction_data_path = \"hdfs://localhost:9000/stress_analyse/input_/output_messages.csv\"\n",
    "df_predict = spark.read.csv(prediction_data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Convert Spark DataFrame to Pandas for easier processing\n",
    "prediction_pd = df_predict.select(\"text\").toPandas()\n",
    "\n",
    "# **Make Predictions on the New Data**\n",
    "for text in prediction_pd[\"text\"]:\n",
    "    prediction = predict_stress(text)\n",
    "    print(f\"Text: {text} --> Prediction: {prediction}\\n\")\n",
    "\n",
    "    if prediction == \"Stressed\":\n",
    "        print(\"\\033[91mALERT: The text is classified as 'Stressed'!\\033[0m\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec568cab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1633b5a1",
   "metadata": {},
   "source": [
    "## Electra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b15d1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='198' max='198' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [198/198 00:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.674365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.673300</td>\n",
       "      <td>0.643821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.673300</td>\n",
       "      <td>0.625202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6667\n",
      "F1-score: 0.6986\n",
      "Precision: 0.6071\n",
      "Recall: 0.8226\n",
      "AUC-ROC: 0.6756\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: hello --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Trying to decide between a mental breakdown or a stress nap...  #LifeChoices #Overthinking #Stressed\"\"  (A funny --> Prediction: Not Stressed\n",
      "\n",
      "Text: Currently --> Prediction: Stressed\n",
      "\n",
      "\u001b[91mALERT: The text is classified as 'Stressed'!\u001b[0m\n",
      "\n",
      "Text: \"That 3 AM realization that the deadline is in 3 hours... ðŸ˜… Time to activate superhuman focus mode. #DeadlineStress #LateNightWork #FocusMode\"\" ðŸ“¸ (Image of a clock showing the time or a shot of your workspace at night)\" --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Stress level: 100. But at least I can laugh about it --> Prediction: Not Stressed\n",
      "\n",
      "Text: \"\"\"Grateful for all the amazing things happening around me! Learning --> Prediction: Not Stressed\n",
      "\n",
      "Text: ðŸ“¸ (Image of you working or something that represents your current journey)\" --> Prediction: Not Stressed\n",
      "\n",
      "Text: I'm feeling exhausted and can't focus. --> Prediction: Stressed\n",
      "\n",
      "\u001b[91mALERT: The text is classified as 'Stressed'!\u001b[0m\n",
      "\n",
      "Text: I had a peaceful walk by the beach. --> Prediction: Not Stressed\n",
      "\n",
      "Text: Deadlines are piling up, and I feel overwhelmed. --> Prediction: Stressed\n",
      "\n",
      "\u001b[91mALERT: The text is classified as 'Stressed'!\u001b[0m\n",
      "\n",
      "Text: Iâ€™m excited to start my new project! --> Prediction: Not Stressed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"StressDetection\").getOrCreate()\n",
    "\n",
    "# Load data from HDFS\n",
    "data_path = \"hdfs://localhost:9000/inputs/dreaddit_StressAnalysis-Sheet1.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Clean and preprocess labels\n",
    "df = df.withColumn(\"label\", when(col(\"label\") == \"0\", 0).when(col(\"label\") == \"1\", 1).otherwise(None))\n",
    "df = df.withColumn(\"label\", col(\"label\").cast(\"integer\"))\n",
    "df = df.na.drop(subset=[\"label\"])\n",
    "\n",
    "# Convert Spark DataFrame to Pandas\n",
    "data_pd = df.select(\"text\", \"label\").toPandas()\n",
    "\n",
    "# Choose a model\n",
    "model_name = \"google/electra-small-discriminator\"  # Using ELECTRA for efficient classification\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize text data\n",
    "encodings = tokenizer(list(data_pd[\"text\"]), truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "labels = list(data_pd[\"label\"])\n",
    "\n",
    "# Create dataset class\n",
    "class StressDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "dataset = StressDataset(encodings, labels)\n",
    "\n",
    "# Split dataset into training and testing\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,  # Reduced batch size for memory efficiency\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True  # Mixed precision for better performance\n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# **Evaluate the model**\n",
    "results = trainer.predict(test_dataset)\n",
    "y_pred = torch.argmax(torch.tensor(results.predictions), dim=-1).tolist()\n",
    "y_true = [data['labels'].item() for data in test_dataset]\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "auc_roc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "# Display results\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# **Prediction function**\n",
    "def predict_stress(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    pred_label = torch.argmax(probs).item()\n",
    "    return \"Stressed\" if pred_label == 1 else \"Not Stressed\"\n",
    "\n",
    "# **Load New Data for Prediction**\n",
    "prediction_data_path = \"hdfs://localhost:9000/stress_analyse/input_/output_messages.csv\"\n",
    "df_predict = spark.read.csv(prediction_data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Convert Spark DataFrame to Pandas for easier processing\n",
    "prediction_pd = df_predict.select(\"text\").toPandas()\n",
    "\n",
    "# **Make Predictions on the New Data**\n",
    "for text in prediction_pd[\"text\"]:\n",
    "    prediction = predict_stress(text)\n",
    "    print(f\"Text: {text} --> Prediction: {prediction}\\n\")\n",
    "\n",
    "    if prediction == \"Stressed\":\n",
    "        print(\"\\033[91mALERT: The text is classified as 'Stressed'!\\033[0m\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652564e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b088b4a0",
   "metadata": {},
   "source": [
    "### Fine tune Electra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97811d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/jyoti/fastapi-env/lib/python3.9/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='99' max='99' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [99/99 00:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.686057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.678888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.675977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[94mModel Evaluation Metrics:\u001b[0m\n",
      "ðŸ”¹ Accuracy: 0.6742\n",
      "ðŸ”¹ F1-score: 0.7571\n",
      "ðŸ”¹ Precision: 0.6204\n",
      "ðŸ”¹ Recall: 0.9710\n",
      "ðŸ”¹ AUC-ROC: 0.6601\n",
      "\n",
      "\u001b[94mStress Prediction Results:\u001b[0m\n",
      "\n",
      "ðŸ”¸ Text: hello \n",
      "   â†’ Prediction: Not Stressed\n",
      "\n",
      "ðŸ”¸ Text: hello \n",
      "   â†’ Prediction: Not Stressed\n",
      "\n",
      "ðŸ”¸ Text: \"\"\"Trying to decide between a mental breakdown or a stress nap...  #LifeChoices #Overthinking #Stressed\"\"  (A funny \n",
      "   â†’ Prediction: Not Stressed\n",
      "\n",
      "ðŸ”¸ Text: Currently \n",
      "   â†’ Prediction: Stressed\n",
      "\n",
      "\u001b[91m ALERT: This text is classified as 'Stressed'!\u001b[0m\n",
      "\n",
      "ðŸ”¸ Text: \"That 3 AM realization that the deadline is in 3 hours... ðŸ˜… Time to activate superhuman focus mode. #DeadlineStress #LateNightWork #FocusMode\"\" ðŸ“¸ (Image of a clock showing the time or a shot of your workspace at night)\" \n",
      "   â†’ Prediction: Not Stressed\n",
      "\n",
      "ðŸ”¸ Text: \"\"\"Stress level: 100. But at least I can laugh about it \n",
      "   â†’ Prediction: Not Stressed\n",
      "\n",
      "ðŸ”¸ Text: \"\"\"Grateful for all the amazing things happening around me! Learning \n",
      "   â†’ Prediction: Stressed\n",
      "\n",
      "\u001b[91m ALERT: This text is classified as 'Stressed'!\u001b[0m\n",
      "\n",
      "ðŸ”¸ Text: ðŸ“¸ (Image of you working or something that represents your current journey)\" \n",
      "   â†’ Prediction: Not Stressed\n",
      "\n",
      "ðŸ”¸ Text: I'm feeling exhausted and can't focus. \n",
      "   â†’ Prediction: Stressed\n",
      "\n",
      "\u001b[91m ALERT: This text is classified as 'Stressed'!\u001b[0m\n",
      "\n",
      "ðŸ”¸ Text: I had a peaceful walk by the beach. \n",
      "   â†’ Prediction: Not Stressed\n",
      "\n",
      "ðŸ”¸ Text: Deadlines are piling up, and I feel overwhelmed. \n",
      "   â†’ Prediction: Stressed\n",
      "\n",
      "\u001b[91m ALERT: This text is classified as 'Stressed'!\u001b[0m\n",
      "\n",
      "ðŸ”¸ Text: Iâ€™m excited to start my new project! \n",
      "   â†’ Prediction: Stressed\n",
      "\n",
      "\u001b[91m ALERT: This text is classified as 'Stressed'!\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"StressDetection\").getOrCreate()\n",
    "\n",
    "# Load data from HDFS\n",
    "data_path = \"hdfs://localhost:9000/inputs/dreaddit_StressAnalysis-Sheet1.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Clean and preprocess labels\n",
    "df = df.withColumn(\"label\", when(col(\"label\") == \"0\", 0).when(col(\"label\") == \"1\", 1).otherwise(None))\n",
    "df = df.withColumn(\"label\", col(\"label\").cast(\"integer\"))\n",
    "df = df.na.drop(subset=[\"label\"])\n",
    "\n",
    "# Convert Spark DataFrame to Pandas\n",
    "data_pd = df.select(\"text\", \"label\").toPandas()\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"google/electra-small-discriminator\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Create dataset class\n",
    "class StressDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=128)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Prepare dataset\n",
    "dataset = StressDataset(data_pd[\"text\"].tolist(), data_pd[\"label\"].tolist())\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,  # Keep only the last 2 checkpoints\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    report_to=\"none\"  # Disable logging to external services\n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.predict(test_dataset)\n",
    "y_pred = torch.argmax(torch.tensor(results.predictions), dim=-1).tolist()\n",
    "y_true = [data[\"labels\"].item() for data in test_dataset]\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "auc_roc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\\033[94mModel Evaluation Metrics:\\033[0m\")\n",
    "print(f\"ðŸ”¹ Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ðŸ”¹ F1-score: {f1:.4f}\")\n",
    "print(f\"ðŸ”¹ Precision: {precision:.4f}\")\n",
    "print(f\"ðŸ”¹ Recall: {recall:.4f}\")\n",
    "print(f\"ðŸ”¹ AUC-ROC: {auc_roc:.4f}\\n\")\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Prediction function\n",
    "def predict_stress(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        pred_label = torch.argmax(probs).item()\n",
    "    \n",
    "    return \"Stressed\" if pred_label == 1 else \"Not Stressed\"\n",
    "\n",
    "# Load new data for prediction\n",
    "prediction_data_path = \"hdfs://localhost:9000/stress_analyse/input_/output_messages.csv\"\n",
    "df_predict = spark.read.csv(prediction_data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Convert Spark DataFrame to Pandas\n",
    "prediction_pd = df_predict.select(\"text\").toPandas()\n",
    "\n",
    "# Make predictions on new data\n",
    "print(\"\\033[94mStress Prediction Results:\\033[0m\\n\")\n",
    "for text in prediction_pd[\"text\"]:\n",
    "    prediction = predict_stress(text)\n",
    "    print(f\"ðŸ”¸ Text: {text} \\n   â†’ Prediction: {prediction}\\n\")\n",
    "\n",
    "    # Generate alert if stressed\n",
    "    if prediction == \"Stressed\":\n",
    "        print(\"\\033[91m ALERT: This text is classified as 'Stressed'!\\033[0m\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f176feb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa163ff9",
   "metadata": {},
   "source": [
    "### Fine tune BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e31d5e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/jyoti/fastapi-env/lib/python3.9/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='165' max='165' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [165/165 00:36, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.709467</td>\n",
       "      <td>0.515152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.714000</td>\n",
       "      <td>0.700500</td>\n",
       "      <td>0.537879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.714000</td>\n",
       "      <td>0.693787</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.697100</td>\n",
       "      <td>0.681428</td>\n",
       "      <td>0.568182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.662700</td>\n",
       "      <td>0.666027</td>\n",
       "      <td>0.613636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[94mStress Prediction Results:\u001b[0m\n",
      "\n",
      "ðŸ”¹ Text: hello \n",
      "   â†’ Prediction: Not Stressed\n",
      "\n",
      "ðŸ”¹ Text: hello \n",
      "   â†’ Prediction: Not Stressed\n",
      "\n",
      "ðŸ”¹ Text: \"\"\"Trying to decide between a mental breakdown or a stress nap...  #LifeChoices #Overthinking #Stressed\"\"  (A funny \n",
      "   â†’ Prediction: Not Stressed\n",
      "\n",
      "ðŸ”¹ Text: Currently \n",
      "   â†’ Prediction: Not Stressed\n",
      "\n",
      "ðŸ”¹ Text: \"That 3 AM realization that the deadline is in 3 hours... ðŸ˜… Time to activate superhuman focus mode. #DeadlineStress #LateNightWork #FocusMode\"\" ðŸ“¸ (Image of a clock showing the time or a shot of your workspace at night)\" \n",
      "   â†’ Prediction: Not Stressed\n",
      "\n",
      "ðŸ”¹ Text: \"\"\"Stress level: 100. But at least I can laugh about it \n",
      "   â†’ Prediction: Stressed\n",
      "\n",
      "\u001b[91m ALERT: This text is classified as 'Stressed'!\u001b[0m\n",
      "\n",
      "ðŸ”¹ Text: \"\"\"Grateful for all the amazing things happening around me! Learning \n",
      "   â†’ Prediction: Stressed\n",
      "\n",
      "\u001b[91m ALERT: This text is classified as 'Stressed'!\u001b[0m\n",
      "\n",
      "ðŸ”¹ Text: ðŸ“¸ (Image of you working or something that represents your current journey)\" \n",
      "   â†’ Prediction: Not Stressed\n",
      "\n",
      "ðŸ”¹ Text: I'm feeling exhausted and can't focus. \n",
      "   â†’ Prediction: Stressed\n",
      "\n",
      "\u001b[91m ALERT: This text is classified as 'Stressed'!\u001b[0m\n",
      "\n",
      "ðŸ”¹ Text: I had a peaceful walk by the beach. \n",
      "   â†’ Prediction: Not Stressed\n",
      "\n",
      "ðŸ”¹ Text: Deadlines are piling up, and I feel overwhelmed. \n",
      "   â†’ Prediction: Not Stressed\n",
      "\n",
      "ðŸ”¹ Text: Iâ€™m excited to start my new project! \n",
      "   â†’ Prediction: Not Stressed\n",
      "\n",
      "\n",
      "\u001b[94mModel Evaluation Metrics:\u001b[0m\n",
      "ðŸ”¹ Accuracy: 0.6136\n",
      "ðŸ”¹ F1-score: 0.6277\n",
      "ðŸ”¹ Precision: 0.5733\n",
      "ðŸ”¹ Recall: 0.6935\n",
      "ðŸ”¹ AUC-ROC: 0.6182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Load model & tokenizer\n",
    "model_name = \"bert-base-uncased\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Dataset Class\n",
    "class StressDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=\"longest\", max_length=128)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Compute class weights for imbalanced dataset\n",
    "class_weights = compute_class_weight(\"balanced\", classes=np.array([0,1]), y=data_pd[\"label\"])\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(\"cuda\")\n",
    "\n",
    "# Split dataset\n",
    "dataset = StressDataset(data_pd[\"text\"].tolist(), data_pd[\"label\"].tolist())\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Use class weights in loss function\n",
    "from torch.nn import CrossEntropyLoss\n",
    "loss_fn = CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Training arguments (adjusted)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,  # More epochs\n",
    "    logging_steps=50,\n",
    "    learning_rate=5e-6,  # Lower learning rate for better fine-tuning\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Define Trainer with weighted loss\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=lambda pred: {\"accuracy\": (pred.predictions.argmax(-1) == pred.label_ids).mean()}\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define prediction function\n",
    "def predict_stress(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    pred_label = torch.argmax(probs).item()\n",
    "    \n",
    "    return \"Stressed\" if pred_label == 1 else \"Not Stressed\"\n",
    "\n",
    "# Load new data for prediction\n",
    "prediction_data_path = \"hdfs://localhost:9000/stress_analyse/input_/output_messages.csv\"\n",
    "df_predict = spark.read.csv(prediction_data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Convert Spark DataFrame to Pandas\n",
    "prediction_pd = df_predict.select(\"text\").toPandas()\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\n\\033[94mStress Prediction Results:\\033[0m\\n\")\n",
    "for text in prediction_pd[\"text\"]:\n",
    "    prediction = predict_stress(text)\n",
    "    print(f\"ðŸ”¹ Text: {text} \\n   â†’ Prediction: {prediction}\\n\")\n",
    "\n",
    "    if prediction == \"Stressed\":\n",
    "        print(\"\\033[91m ALERT: This text is classified as 'Stressed'!\\033[0m\\n\")\n",
    "\n",
    "\n",
    "# **Evaluation Function with Accuracy, F1-score, Precision, Recall, and AUC-ROC**\n",
    "def evaluate_model(test_dataset):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "    \n",
    "    for batch in test_loader:\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != \"labels\"}\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        preds = torch.argmax(outputs.logits, dim=-1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Compute metrics\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    auc_roc = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "    return acc, f1, precision, recall, auc_roc\n",
    "\n",
    "# Compute final metrics\n",
    "final_accuracy, final_f1, final_precision, final_recall, final_auc = evaluate_model(test_dataset)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\n\\033[94mModel Evaluation Metrics:\\033[0m\")\n",
    "print(f\"ðŸ”¹ Accuracy: {final_accuracy:.4f}\")\n",
    "print(f\"ðŸ”¹ F1-score: {final_f1:.4f}\")\n",
    "print(f\"ðŸ”¹ Precision: {final_precision:.4f}\")\n",
    "print(f\"ðŸ”¹ Recall: {final_recall:.4f}\")\n",
    "print(f\"ðŸ”¹ AUC-ROC: {final_auc:.4f}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastapi-env)",
   "language": "python",
   "name": "fastapi-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
